\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{li2020fourier}
\citation{potter2017pkdgrav3}
\citation{potter2017pkdgrav3,springel2021simulating,garrison2021abacus}
\citation{jasche2013bayesian}
\citation{jasche2013bayesian}
\citation{djolonga2013high}
\citation{wang2014elucid}
\citation{potter2017pkdgrav3}
\citation{li2020fourier}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}{section.1}\protected@file@percent }
\citation{carroll2007dark}
\citation{schutz2003gravity}
\@writefile{toc}{\contentsline {section}{\numberline {2}Dark Matter N-Body Simulations}{6}{section.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Comoving Coordinates}{6}{subsection.2.1}\protected@file@percent }
\citation{bars2010extra}
\citation{cui2008ideal}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Time}{7}{subsection.2.2}\protected@file@percent }
\newlabel{eq:redshift}{{4}{7}{Time}{equation.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Units and Scale}{7}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Power Spectrum}{7}{subsection.2.4}\protected@file@percent }
\newlabel{power-spectrum}{{2.4}{7}{Power Spectrum}{subsection.2.4}{}}
\citation{schneider1995power}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Different power spectrum approximations for redshifts 49, 10, 3, and 0 (present day): linear, nonlinear, and Zel'dovich approximations.}}{8}{figure.caption.1}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:power-z}{{1}{8}{Different power spectrum approximations for redshifts 49, 10, 3, and 0 (present day): linear, nonlinear, and Zel'dovich approximations}{figure.caption.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Mass Assignment}{8}{subsection.2.5}\protected@file@percent }
\newlabel{mass-assignment}{{2.5}{8}{Mass Assignment}{subsection.2.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces CIC mass assignment from cosmax.}}{9}{figure.caption.2}\protected@file@percent }
\newlabel{fig:cosmax-cic}{{2}{9}{CIC mass assignment from cosmax}{figure.caption.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.6}Governing Equations}{9}{subsection.2.6}\protected@file@percent }
\newlabel{eq:velocity}{{8}{9}{Governing Equations}{equation.8}{}}
\newlabel{eq:potential-poisson}{{9}{9}{Governing Equations}{equation.9}{}}
\citation{bertschinger1995cosmics}
\citation{carroll2001cosmological}
\citation{carroll2017introduction}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.7}Initial Conditions}{10}{subsection.2.7}\protected@file@percent }
\newlabel{eq:growth}{{12}{10}{Initial Conditions}{equation.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.7.1}IC Generation}{10}{subsubsection.2.7.1}\protected@file@percent }
\newlabel{IC-GEN}{{2.7.1}{10}{IC Generation}{subsubsection.2.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces Random distribution, its power spectrum, and the desired power spectrum.}}{10}{figure.caption.3}\protected@file@percent }
\newlabel{fig:random}{{3}{10}{Random distribution, its power spectrum, and the desired power spectrum}{figure.caption.3}{}}
\citation{prunet2008initial}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Random distribution, its power spectrum, and the desired power spectrum.}}{11}{figure.caption.4}\protected@file@percent }
\newlabel{fig:cocrr}{{4}{11}{Random distribution, its power spectrum, and the desired power spectrum}{figure.caption.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.8}N-Body Simulations}{11}{subsection.2.8}\protected@file@percent }
\newlabel{nbody}{{2.8}{11}{N-Body Simulations}{subsection.2.8}{}}
\citation{demmel1996solving,mccormick1987multigrid}
\citation{barnes1986hierarchical}
\citation{rokhlin1985rapid}
\citation{potter2017pkdgrav3}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces The resulting density field, its power spectrum and the desired power spectrum.}}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig:result}{{5}{12}{The resulting density field, its power spectrum and the desired power spectrum}{figure.caption.5}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.1}Particle Mesh Method}{12}{subsubsection.2.8.1}\protected@file@percent }
\newlabel{pm}{{2.8.1}{12}{Particle Mesh Method}{subsubsection.2.8.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.8.2}Fast Multipole Method}{12}{subsubsection.2.8.2}\protected@file@percent }
\newlabel{fmm}{{2.8.2}{12}{Fast Multipole Method}{subsubsection.2.8.2}{}}
\citation{thuerey2021physics}
\citation{jiang2023mistral}
\citation{kingma2014adam}
\citation{thuerey2021physics}
\@writefile{toc}{\contentsline {section}{\numberline {3}Neural Networks and Optimization}{13}{section.3}\protected@file@percent }
\newlabel{nno}{{3}{13}{Neural Networks and Optimization}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Gradient-Based Optimization}{13}{subsection.3.1}\protected@file@percent }
\citation{bottou2007tradeoffs}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Automatic Differentiation}{14}{subsection.3.2}\protected@file@percent }
\newlabel{ad}{{3.2}{14}{Automatic Differentiation}{subsection.3.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Discrete Convolutional Operator}{15}{subsection.3.3}\protected@file@percent }
\newlabel{eq:bounded-domain}{{33}{15}{Discrete Convolutional Operator}{equation.33}{}}
\citation{lecun1998gradient}
\citation{krizhevsky2012imagenet}
\citation{ronneberger2015u}
\citation{bernardini2022ember}
\citation{ronneberger2015u}
\citation{ronneberger2015u}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}UNet}{16}{subsection.3.4}\protected@file@percent }
\citation{mcgreivy2024weak}
\citation{li2020fourier}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Architecture of the original UNet as proposed by \cite  {ronneberger2015u}. A maxpooling layer, is a convolutional layer with a stride, which in this case is always two. An up.conv layer is a transposed convolutional layer with stride 2. }}{17}{figure.caption.6}\protected@file@percent }
\newlabel{fig:unet}{{6}{17}{Architecture of the original UNet as proposed by \cite {ronneberger2015u}. A maxpooling layer, is a convolutional layer with a stride, which in this case is always two. An up.conv layer is a transposed convolutional layer with stride 2}{figure.caption.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Spectral Convolution}{17}{subsection.3.5}\protected@file@percent }
\citation{li2020fourier}
\citation{li2020fourier}
\citation{li2020fourier}
\citation{koehler2024apebench}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}FNO Architecture}{18}{subsection.3.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces (a) The entire Fourier Neural Operator architecture, (b) The Fourier Layer architecture. Image sourced from \cite  {li2020fourier}.}}{18}{figure.caption.7}\protected@file@percent }
\newlabel{fig:fno-architecture}{{7}{18}{(a) The entire Fourier Neural Operator architecture, (b) The Fourier Layer architecture. Image sourced from \cite {li2020fourier}}{figure.caption.7}{}}
\citation{jasche2013bayesian}
\@writefile{toc}{\contentsline {section}{\numberline {4}Problem Statement and Related Work}{20}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Inverse Problem}{20}{subsection.4.1}\protected@file@percent }
\citation{nusser1992tracing}
\citation{jasche2013bayesian}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces Exploration of error in reverse time integration for N-Body system. The particles are integrated forwards in time using the leap frog integration scheme followed by a reverse time integration, also with leap frog integration but reversed forces. The error measures the mean of the squared defects between the particle's initial positions and their final positions after forwards and backwards integration. Parameters, if not declared otherwise in the plot are, $\Delta t= 0.1$ , $N = 100$, $n = 100$. The gravity forces are evaluated directly without any optimization, leveraging a brute force $O(N^2)$ approach. The simulation was done in a cubic periodic box of length one with a random, uniform initial particle distribution. To smoothen to avoid highly non-linear forces, a softening length of 0.01 was used. A more detailed description of the code can be found in section \ref {rev-integration}.}}{21}{figure.caption.8}\protected@file@percent }
\newlabel{fig:error_inverse}{{8}{21}{Exploration of error in reverse time integration for N-Body system. The particles are integrated forwards in time using the leap frog integration scheme followed by a reverse time integration, also with leap frog integration but reversed forces. The error measures the mean of the squared defects between the particle's initial positions and their final positions after forwards and backwards integration. Parameters, if not declared otherwise in the plot are, $\Delta t= 0.1$ , $N = 100$, $n = 100$. The gravity forces are evaluated directly without any optimization, leveraging a brute force $O(N^2)$ approach. The simulation was done in a cubic periodic box of length one with a random, uniform initial particle distribution. To smoothen to avoid highly non-linear forces, a softening length of 0.01 was used. A more detailed description of the code can be found in section \ref {rev-integration}}{figure.caption.8}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Reverse Time Integration}{21}{subsection.4.2}\protected@file@percent }
\citation{li2024differentiable}
\citation{mccormick1987multigrid}
\citation{potter2017pkdgrav3}
\citation{wang2009minimal}
\citation{griewank2000algorithm}
\citation{jasche2013bayesian,kitaura2013initial}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Differentiable Solver for IC Optimization}{22}{subsection.4.3}\protected@file@percent }
\newlabel{IC-opt}{{4.3}{22}{Differentiable Solver for IC Optimization}{subsection.4.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Statistical Methods for Constrained IC's}{22}{subsection.4.4}\protected@file@percent }
\citation{jasche2013bayesian}
\citation{asher2015review}
\citation{forrester2008engineering}
\citation{kadupitiya2020machine}
\citation{mcgreivy2024weak}
\citation{pathak2022fourcastnet}
\citation{liu2023deepoheat}
\citation{li2020fourier}
\citation{long2024invertible}
\citation{mack2007surrogate}
\citation{forrester2009recent}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Surrogate Model for IC Optimizationl}{23}{subsection.4.5}\protected@file@percent }
\citation{koehler2024apebench}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Our Method}{24}{subsection.4.6}\protected@file@percent }
\newlabel{goal}{{59}{24}{Our Method}{equation.59}{}}
\citation{potter2017pkdgrav3}
\citation{jasche2013bayesian,kitaura2013initial}
\citation{jax2018github}
\citation{kingma2014adam}
\citation{qian1999momentum}
\citation{kingma2014adam}
\@writefile{toc}{\contentsline {section}{\numberline {5}Experimental Approach and Outcomes}{25}{section.5}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Simulation}{25}{subsection.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Default Hyperparameters}{25}{subsection.5.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Parameters for large and small PKDGRAV3 simulations.}}{26}{table.caption.9}\protected@file@percent }
\newlabel{table:1}{{1}{26}{Parameters for large and small PKDGRAV3 simulations}{table.caption.9}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Normalization}{26}{subsection.5.3}\protected@file@percent }
\citation{patro2015normalization}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Overdensity Normalization}{27}{subsubsection.5.3.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Distribution of overdensity at selected redshifts, illustrating complex structure with subtle and high-valued peaks}}{27}{figure.caption.10}\protected@file@percent }
\newlabel{fig:ditr_delta}{{9}{27}{Distribution of overdensity at selected redshifts, illustrating complex structure with subtle and high-valued peaks}{figure.caption.10}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Standard Score Normalization}{27}{subsubsection.5.3.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces Z-score normalization of the data distribution, facilitating a more uniform variance across the dataset}}{28}{figure.caption.11}\protected@file@percent }
\newlabel{fig:z-score}{{10}{28}{Z-score normalization of the data distribution, facilitating a more uniform variance across the dataset}{figure.caption.11}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Log Growth Normalization}{28}{subsubsection.5.3.3}\protected@file@percent }
\citation{denoising}
\citation{koehler2024apebench}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Log growth normalization, highlighting the linear regimen of early dynamic stages}}{29}{figure.caption.12}\protected@file@percent }
\newlabel{fig:log-growth}{{11}{29}{Log growth normalization, highlighting the linear regimen of early dynamic stages}{figure.caption.12}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Normalization Conclusion}{29}{subsubsection.5.3.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4}Network Architecture}{29}{subsection.5.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.1}UNet}{30}{subsubsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.2}FNO}{31}{subsubsection.5.4.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.3}Adapted FNO}{31}{subsubsection.5.4.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.4.4}Network Architecture Conclusion}{32}{subsubsection.5.4.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.5}Hyperparameters}{32}{subsection.5.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Validation loss against compute time for different channel numbers in FNOs. The corresponding parameter count is denoted in the label of the plot.}}{33}{figure.caption.13}\protected@file@percent }
\newlabel{fig:channels-B}{{12}{33}{Validation loss against compute time for different channel numbers in FNOs. The corresponding parameter count is denoted in the label of the plot}{figure.caption.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Visual and Power Spectrum comparison of predicted density fields with FNOs of different channel numbers. The top row shows the residual between the predictions and the ground truth. Red areas highlight regions where the prediction deposits too much mass, while blue areas indicate the opposite.}}{33}{figure.caption.14}\protected@file@percent }
\newlabel{fig:channels-A}{{13}{33}{Visual and Power Spectrum comparison of predicted density fields with FNOs of different channel numbers. The top row shows the residual between the predictions and the ground truth. Red areas highlight regions where the prediction deposits too much mass, while blue areas indicate the opposite}{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces Validation loss against compute time for different layer counts in FNO's. The corresponding parameter count is denoted in the legend of the plot.}}{34}{figure.caption.15}\protected@file@percent }
\newlabel{fig:layer-B}{{14}{34}{Validation loss against compute time for different layer counts in FNO's. The corresponding parameter count is denoted in the legend of the plot}{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Visual and Power Spectrum comparison of predicted density fields with FNO's of different different FNO layer counts. The top column shows the residual between the predicted and the ground truth. Red areas highlight areas where the prediction deposits too much mass, blue areas the opposite. }}{34}{figure.caption.16}\protected@file@percent }
\newlabel{fig:layer-A}{{15}{34}{Visual and Power Spectrum comparison of predicted density fields with FNO's of different different FNO layer counts. The top column shows the residual between the predicted and the ground truth. Red areas highlight areas where the prediction deposits too much mass, blue areas the opposite}{figure.caption.16}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.6}FNO Capabilities}{34}{subsection.5.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Top tow shows input distribution where k modes greater than 1 $h Mpc^{-1}$ are removed. The bottom row shows the prediction, where on very left is the ground truth of the simulation.}}{35}{figure.caption.17}\protected@file@percent }
\newlabel{fig:filter}{{16}{35}{Top tow shows input distribution where k modes greater than 1 $h Mpc^{-1}$ are removed. The bottom row shows the prediction, where on very left is the ground truth of the simulation}{figure.caption.17}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.6.1}Prediction Time Intervals}{35}{subsubsection.5.6.1}\protected@file@percent }
\newlabel{timeintervals}{{5.6.1}{35}{Prediction Time Intervals}{subsubsection.5.6.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Prediction from $z=4.25$ to $z=49$ trained on the large dataset using a 5 layer adapted FNO and MSE.}}{36}{figure.caption.18}\protected@file@percent }
\newlabel{fig:low-modes}{{17}{36}{Prediction from $z=4.25$ to $z=49$ trained on the large dataset using a 5 layer adapted FNO and MSE}{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Comparison of identical FNO models trained on different time regions.}}{36}{figure.caption.19}\protected@file@percent }
\newlabel{fig:IC-comp}{{18}{36}{Comparison of identical FNO models trained on different time regions}{figure.caption.19}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.7}Further Enhancements}{36}{subsection.5.7}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.1}Power Spectrum Loss}{36}{subsubsection.5.7.1}\protected@file@percent }
\newlabel{ps-loss}{{5.7.1}{36}{Power Spectrum Loss}{subsubsection.5.7.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces Comparison of identical FNO models trained on different time regions.}}{37}{figure.caption.20}\protected@file@percent }
\newlabel{fig:ranges}{{19}{37}{Comparison of identical FNO models trained on different time regions}{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces Prediction from $z=19.2$ to $z=49$, compared with or without additional power loss term.}}{37}{figure.caption.21}\protected@file@percent }
\newlabel{fig:power-loss}{{20}{37}{Prediction from $z=19.2$ to $z=49$, compared with or without additional power loss term}{figure.caption.21}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces Prediction from $z=19.2$ to $z=49$, compared with or without additional power loss term.}}{38}{figure.caption.22}\protected@file@percent }
\newlabel{fig:power-loss-corr}{{21}{38}{Prediction from $z=19.2$ to $z=49$, compared with or without additional power loss term}{figure.caption.22}{}}
\citation{kaplan2020scaling}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.7.2}Gravitational Potential Feature}{39}{subsubsection.5.7.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Prediction from $z=19.2$ to $z=49$. Comparison of identical FNO networks trained on the density fields, with and without the potential as an additional input channel.}}{39}{figure.caption.23}\protected@file@percent }
\newlabel{fig:potential}{{22}{39}{Prediction from $z=19.2$ to $z=49$. Comparison of identical FNO networks trained on the density fields, with and without the potential as an additional input channel}{figure.caption.23}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.8}Large Dataset Training}{39}{subsection.5.8}\protected@file@percent }
\citation{brandstetter2022message}
\citation{brandstetter2022message}
\citation{li2020fourier}
\newlabel{fig:large-A}{{23a}{40}{Subfigure 23a}{subfigure.23.1}{}}
\newlabel{sub@fig:large-A}{{(a)}{a}{Subfigure 23a\relax }{subfigure.23.1}{}}
\newlabel{fig:large-B}{{23b}{40}{Subfigure 23b}{subfigure.23.2}{}}
\newlabel{sub@fig:large-B}{{(b)}{b}{Subfigure 23b\relax }{subfigure.23.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces Training and prediction on large dataset.}}{40}{figure.caption.24}\protected@file@percent }
\newlabel{fig:large}{{23}{40}{Training and prediction on large dataset}{figure.caption.24}{}}
\@writefile{lof}{\contentsline {subfigure}{\numberline{(a)}{\ignorespaces {Results from network trained on the range of $z=0$ to $z=49$.}}}{40}{subfigure.23.1}\protected@file@percent }
\@writefile{lof}{\contentsline {subfigure}{\numberline{(b)}{\ignorespaces {Results from network trained on the range of $z=4.25$ to $z=49$.}}}{40}{subfigure.23.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.9}Auto-regressive Predictions}{40}{subsection.5.9}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Different training schemes. Figure from \cite  {brandstetter2022message}.}}{40}{figure.caption.25}\protected@file@percent }
\newlabel{fig:schemes-training}{{24}{40}{Different training schemes. Figure from \cite {brandstetter2022message}}{figure.caption.25}{}}
\citation{brandstetter2022message}
\citation{krishnapriyan2021characterizing}
\newlabel{goal-2}{{77}{41}{Auto-regressive Predictions}{equation.77}{}}
\newlabel{goal-rec}{{78}{41}{Auto-regressive Predictions}{equation.78}{}}
\newlabel{step}{{80}{41}{Auto-regressive Predictions}{equation.80}{}}
\newlabel{seq}{{81}{41}{Auto-regressive Predictions}{equation.81}{}}
\citation{thuerey2021physics}
\citation{zhang2019gradient}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.9.1}Unique Network Parametrization}{42}{subsubsection.5.9.1}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Different training schemes for unique network parametrization. Here the blue boxes denote the sequence of ground truths, the orange boxes are the predictions, green is the loss and and purple denotes the parameters of the invidivual neural networks.}}{42}{figure.caption.26}\protected@file@percent }
\newlabel{fig:autoreg}{{25}{42}{Different training schemes for unique network parametrization. Here the blue boxes denote the sequence of ground truths, the orange boxes are the predictions, green is the loss and and purple denotes the parameters of the invidivual neural networks}{figure.caption.26}{}}
\newlabel{mix}{{83}{42}{Unique Network Parametrization}{equation.83}{}}
\citation{li2020fourier,gopakumar2023fourier}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Multistep Prediction on the large dataset using an FNO.}}{43}{figure.caption.27}\protected@file@percent }
\newlabel{fig:multistep}{{26}{43}{Multistep Prediction on the large dataset using an FNO}{figure.caption.27}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.9.2}Skip Connections}{43}{subsubsection.5.9.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces Three-step prediction using an adaptive FNO with a three channel skip connections on the large dataset.}}{44}{figure.caption.28}\protected@file@percent }
\newlabel{fig:mss}{{27}{44}{Three-step prediction using an adaptive FNO with a three channel skip connections on the large dataset}{figure.caption.28}{}}
\newlabel{equation-corr}{{84}{45}{Skip Connections}{equation.84}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces Three-step prediction using an adaptive FNO with a three channel skip connections on the large dataset. In the top row: Comparision of input, output and ground truth from sample from validation set. Bottom left power spectrum and Power spectrum correlation of the prediction. The prediction was made over 20 samples, where the grey area indicates the minimum and maximum range the power spectrum correlation reaches. Bottom right: Correlation as evaluated with the equation \ref {equation-corr}. Each line represents a different sample, where in total 20 samples are used.}}{45}{figure.caption.29}\protected@file@percent }
\newlabel{fig:eval-corr}{{28}{45}{Three-step prediction using an adaptive FNO with a three channel skip connections on the large dataset. In the top row: Comparision of input, output and ground truth from sample from validation set. Bottom left power spectrum and Power spectrum correlation of the prediction. The prediction was made over 20 samples, where the grey area indicates the minimum and maximum range the power spectrum correlation reaches. Bottom right: Correlation as evaluated with the equation \ref {equation-corr}. Each line represents a different sample, where in total 20 samples are used}{figure.caption.29}{}}
\citation{enzyme1,enzyme2,enzyme3}
\citation{jax2018github}
\citation{dion_hafner_2021_5607491}
\citation{NEURIPS2020_9332c513}
\citation{kidger2021equinox}
\citation{flax2020github}
\citation{deepmind2020jax}
\citation{kingma2014adam}
\citation{pytestx.y}
\@writefile{toc}{\contentsline {section}{\numberline {6}Implementation}{46}{section.6}\protected@file@percent }
\newlabel{impl}{{6}{46}{Implementation}{section.6}{}}
\citation{jax2018github}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Model Serialization}{47}{subsection.6.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Parallelization}{47}{subsection.6.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Differentiable Power Spectrum}{47}{subsection.6.3}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Configuration parameters for the volumetric training pipeline.}}{48}{table.caption.30}\protected@file@percent }
\newlabel{table:config}{{2}{48}{Configuration parameters for the volumetric training pipeline}{table.caption.30}{}}
\citation{jax2018github}
\citation{murray2018powerbox}
\citation{murray2018powerbox}
\citation{frigo1999fftw}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Performance}{50}{subsubsection.6.3.1}\protected@file@percent }
\citation{jax2018github}
\citation{kidger2021equinox}
\citation{machinelearningsimulation,kossaifi2024neural}
\citation{li2020fourier}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Power spectrum computed on an overdensity field at $z=49$.}}{51}{figure.caption.31}\protected@file@percent }
\newlabel{fig:powerspectrum}{{29}{51}{Power spectrum computed on an overdensity field at $z=49$}{figure.caption.31}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Cosmax power spectrum benchmark.}}{51}{figure.caption.32}\protected@file@percent }
\newlabel{fig:cosmax-bench}{{30}{51}{Cosmax power spectrum benchmark}{figure.caption.32}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.4}3D FNO}{51}{subsection.6.4}\protected@file@percent }
\citation{kossaifi2024neural}
\citation{kossaifi2024neural}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces Spectral Convolution without and with Fourier Space Shifting. Figure from \cite  {kossaifi2024neural}.}}{52}{figure.caption.33}\protected@file@percent }
\newlabel{fig:shifting}{{31}{52}{Spectral Convolution without and with Fourier Space Shifting. Figure from \cite {kossaifi2024neural}}{figure.caption.33}{}}
\newlabel{spectral-conv}{{85}{52}{3D FNO}{equation.85}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusion}{54}{section.7}\protected@file@percent }
\citation{hut1995building}
\@writefile{toc}{\contentsline {section}{Appendices}{56}{section*.34}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {A}Exploratory Work in Differentiable Physics}{56}{appendix.1.A}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {A.1}Reverse Time Integration}{56}{subsection.1.A.1}\protected@file@percent }
\newlabel{rev-integration}{{A.1}{56}{Reverse Time Integration}{subsection.1.A.1}{}}
\citation{enzyme1,enzyme2,enzyme3}
\citation{li2024differentiable}
\citation{chen2018neural}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.2}Differentiable Physics with Enzyme}{57}{subsection.1.A.2}\protected@file@percent }
\newlabel{diff-julia}{{A.2}{57}{Differentiable Physics with Enzyme}{subsection.1.A.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces A particle system with ten attractors and, 10000 particles. The left plot shows the initial state, the right plot shows the state after a set number of iterations.}}{57}{figure.caption.35}\protected@file@percent }
\newlabel{fig:enzyme}{{32}{57}{A particle system with ten attractors and, 10000 particles. The left plot shows the initial state, the right plot shows the state after a set number of iterations}{figure.caption.35}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {A.3}Adjoint ODE}{57}{subsection.1.A.3}\protected@file@percent }
\newlabel{adj-ode}{{A.3}{57}{Adjoint ODE}{subsection.1.A.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces Learning the dynamics of an ODE using AD. Left image shows the trajectory of the ODE with the true dynamics, right plot shows the trajectory of an ODE whose dynamics were learned with the adjoint ODE equation.}}{58}{figure.caption.36}\protected@file@percent }
\newlabel{fig:ode-ad}{{33}{58}{Learning the dynamics of an ODE using AD. Left image shows the trajectory of the ODE with the true dynamics, right plot shows the trajectory of an ODE whose dynamics were learned with the adjoint ODE equation}{figure.caption.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Learning the dynamics of an ODE using adjoint ODE. Left image shows the trajectory of the ODE with the true dynamics, right plot shows the trajectory of an ODE whose dynamics were learned with the adjoint ODE equation.}}{59}{figure.caption.37}\protected@file@percent }
\newlabel{fig:ode-adjoint}{{34}{59}{Learning the dynamics of an ODE using adjoint ODE. Left image shows the trajectory of the ODE with the true dynamics, right plot shows the trajectory of an ODE whose dynamics were learned with the adjoint ODE equation}{figure.caption.37}{}}
\citation{bernardini2022ember}
\citation{gauthier2014conditional}
\citation{jax2018github}
\citation{kidger2021equinox}
\@writefile{toc}{\contentsline {section}{\numberline {B}Other Neural Network Architectures}{60}{appendix.1.B}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {B.1}Generative Adversial Networks}{60}{subsection.1.B.1}\protected@file@percent }
\newlabel{CGAN}{{B.1}{60}{Generative Adversial Networks}{subsection.1.B.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Loss values of actors and critics during the training of a GAN on an art dataset.}}{60}{figure.caption.38}\protected@file@percent }
\newlabel{fig:gan}{{35}{60}{Loss values of actors and critics during the training of a GAN on an art dataset}{figure.caption.38}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {B.2}Encoder Decoder Methods}{60}{subsection.1.B.2}\protected@file@percent }
\newlabel{encoder-decoder}{{B.2}{60}{Encoder Decoder Methods}{subsection.1.B.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Decoding images into latent space and encoding them using trained decoder and encoder networks. Top row shows the original image, bottom row shows an image that was compressed into a latent space with 10\% of the information density.}}{61}{figure.caption.39}\protected@file@percent }
\newlabel{fig:ed}{{36}{61}{Decoding images into latent space and encoding them using trained decoder and encoder networks. Top row shows the original image, bottom row shows an image that was compressed into a latent space with 10\% of the information density}{figure.caption.39}{}}
\@writefile{toc}{\contentsline {section}{\numberline {C}IC Generation}{61}{appendix.1.C}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {C.1}Optimizing over the Power Spectrum}{61}{subsection.1.C.1}\protected@file@percent }
\newlabel{optim}{{C.1}{61}{Optimizing over the Power Spectrum}{subsection.1.C.1}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Initial conditions as generated with cosmax.}}{61}{figure.caption.40}\protected@file@percent }
\newlabel{fig:cosmax-ic}{{37}{61}{Initial conditions as generated with cosmax}{figure.caption.40}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {C.2}Conditional White Noise}{61}{subsection.1.C.2}\protected@file@percent }
\newlabel{white-noise}{{C.2}{61}{Conditional White Noise}{subsection.1.C.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Power Spectrum as a loss function to generate conditional IC.}}{62}{figure.caption.41}\protected@file@percent }
\newlabel{fig:cosmax-cosmax}{{38}{62}{Power Spectrum as a loss function to generate conditional IC}{figure.caption.41}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Power Spectrum as a loss function to generate conditional IC.}}{62}{figure.caption.42}\protected@file@percent }
\newlabel{fig:cosmax-white-noise}{{39}{62}{Power Spectrum as a loss function to generate conditional IC}{figure.caption.42}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Power Spectrum as a loss function to generate conditional IC.}}{63}{figure.caption.43}\protected@file@percent }
\newlabel{fig:white-distr}{{40}{63}{Power Spectrum as a loss function to generate conditional IC}{figure.caption.43}{}}
\@writefile{toc}{\contentsline {section}{\numberline {D}Differentiable CIC Mass Assignment}{63}{appendix.1.D}\protected@file@percent }
\newlabel{inverse-map}{{D}{63}{Differentiable CIC Mass Assignment}{appendix.1.D}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces Fitting particles to a density field.}}{65}{figure.caption.44}\protected@file@percent }
\newlabel{fig:cosmax-fit}{{41}{65}{Fitting particles to a density field}{figure.caption.44}{}}
\citation{kingma2014adam}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces Difference between fitted density field an actual density field normalized by dividing through the standard deviation.}}{66}{figure.caption.45}\protected@file@percent }
\newlabel{fig:field-fit}{{42}{66}{Difference between fitted density field an actual density field normalized by dividing through the standard deviation}{figure.caption.45}{}}
\bibstyle{plainnat}
\bibdata{reference}
\bibcite{asher2015review}{{1}{2015}{{Asher et~al.}}{{Asher, Croke, Jakeman, and Peeters}}}
\bibcite{barnes1986hierarchical}{{2}{1986}{{Barnes and Hut}}{{}}}
\bibcite{bars2010extra}{{3}{2010}{{Bars et~al.}}{{Bars, Terning, and Nekoogar}}}
\bibcite{bernardini2022ember}{{4}{2022}{{Bernardini et~al.}}{{Bernardini, Feldmann, Angl{\'e}s-Alc{\'a}zar, Boylan-Kolchin, Bullock, Mayer, and Stadel}}}
\bibcite{bertschinger1995cosmics}{{5}{1995}{{Bertschinger}}{{}}}
\bibcite{bottou2007tradeoffs}{{6}{2007}{{Bottou and Bousquet}}{{}}}
\bibcite{jax2018github}{{7}{2018}{{Bradbury et~al.}}{{Bradbury, Frostig, Hawkins, Johnson, Leary, Maclaurin, Necula, Paszke, Vander{P}las, Wanderman-{M}ilne, and Zhang}}}
\bibcite{brandstetter2022message}{{8}{2022}{{Brandstetter et~al.}}{{Brandstetter, Worrall, and Welling}}}
\bibcite{carroll2017introduction}{{9}{2017}{{Carroll and Ostlie}}{{}}}
\bibcite{carroll2001cosmological}{{10}{2001}{{Carroll}}{{}}}
\bibcite{carroll2007dark}{{11}{2007}{{Carroll}}{{}}}
\bibcite{chen2018neural}{{12}{2018}{{Chen et~al.}}{{Chen, Rubanova, Bettencourt, and Duvenaud}}}
\bibcite{cui2008ideal}{{13}{2008}{{Cui et~al.}}{{Cui, Liu, Yang, Wang, Feng, and Springel}}}
\bibcite{deepmind2020jax}{{14}{2020}{{DeepMind et~al.}}{{DeepMind, Babuschkin, Baumli, Bell, Bhupatiraju, Bruce, Buchlovsky, Budden, Cai, Clark, Danihelka, Dedieu, Fantacci, Godwin, Jones, Hemsley, Hennigan, Hessel, Hou, Kapturowski, Keck, Kemaev, King, Kunesch, Martens, Merzic, Mikulik, Norman, Papamakarios, Quan, Ring, Ruiz, Sanchez, Sartran, Schneider, Sezener, Spencer, Srinivasan, Stanojevi\'{c}, Stokowiec, Wang, Zhou, and Viola}}}
\bibcite{demmel1996solving}{{15}{1996}{{Demmel}}{{}}}
\bibcite{djolonga2013high}{{16}{2013}{{Djolonga et~al.}}{{Djolonga, Krause, and Cevher}}}
\bibcite{forrester2008engineering}{{17}{2008}{{Forrester et~al.}}{{Forrester, Sobester, and Keane}}}
\bibcite{forrester2009recent}{{18}{2009}{{Forrester and Keane}}{{}}}
\bibcite{frigo1999fftw}{{19}{1999}{{Frigo and Johnson}}{{}}}
\bibcite{garrison2021abacus}{{20}{2021}{{Garrison et~al.}}{{Garrison, Eisenstein, Ferrer, Maksimova, and Pinto}}}
\bibcite{gauthier2014conditional}{{21}{2014}{{Gauthier}}{{}}}
\bibcite{gopakumar2023fourier}{{22}{2023}{{Gopakumar et~al.}}{{Gopakumar, Pamela, and Zanisi}}}
\bibcite{griewank2000algorithm}{{23}{2000}{{Griewank and Walther}}{{}}}
\bibcite{flax2020github}{{24}{2024}{{Heek et~al.}}{{Heek, Levskaya, Oliver, Ritter, Rondepierre, Steiner, and van {Z}ee}}}
\bibcite{denoising}{{25}{2020}{{Ho et~al.}}{{Ho, Jain, and Abbeel}}}
\bibcite{hut1995building}{{26}{1995}{{Hut et~al.}}{{Hut, Makino, and McMillan}}}
\bibcite{dion_hafner_2021_5607491}{{27}{2021}{{Häfner and Moldovan}}{{}}}
\bibcite{jasche2013bayesian}{{28}{2013}{{Jasche and Wandelt}}{{}}}
\bibcite{jiang2023mistral}{{29}{2023}{{Jiang et~al.}}{{Jiang, Sablayrolles, Mensch, Bamford, Chaplot, Casas, Bressand, Lengyel, Lample, Saulnier, et~al.}}}
\bibcite{kadupitiya2020machine}{{30}{2020}{{Kadupitiya et~al.}}{{Kadupitiya, Sun, Fox, and Jadhao}}}
\bibcite{kaplan2020scaling}{{31}{2020}{{Kaplan et~al.}}{{Kaplan, McCandlish, Henighan, Brown, Chess, Child, Gray, Radford, Wu, and Amodei}}}
\bibcite{kidger2021equinox}{{32}{2021}{{Kidger and Garcia}}{{}}}
\bibcite{kingma2014adam}{{33}{2014}{{Kingma}}{{}}}
\bibcite{kitaura2013initial}{{34}{2013}{{Kitaura}}{{}}}
\bibcite{koehler2024apebench}{{35}{2024}{{Koehler et~al.}}{{Koehler, Niedermayr, Westermann, and Thuerey}}}
\bibcite{kossaifi2024neural}{{36}{2024}{{Kossaifi et~al.}}{{Kossaifi, Kovachki, Li, Pitt, Liu-Schiaffini, George, Bonev, Azizzadenesheli, Berner, and Anandkumar}}}
\bibcite{pytestx.y}{{37}{2004}{{Krekel et~al.}}{{Krekel, Oliveira, Pfannschmidt, Bruynooghe, Laugher, and Bruhin}}}
\bibcite{krishnapriyan2021characterizing}{{38}{2021}{{Krishnapriyan et~al.}}{{Krishnapriyan, Gholami, Zhe, Kirby, and Mahoney}}}
\bibcite{krizhevsky2012imagenet}{{39}{2012}{{Krizhevsky et~al.}}{{Krizhevsky, Sutskever, and Hinton}}}
\bibcite{machinelearningsimulation}{{40}{}{{Köhler}}{{}}}
\bibcite{lecun1998gradient}{{41}{1998}{{LeCun et~al.}}{{LeCun, Bottou, Bengio, and Haffner}}}
\bibcite{li2024differentiable}{{42}{2024}{{Li et~al.}}{{Li, Modi, Jamieson, Zhang, Lu, Feng, Lanusse, and Greengard}}}
\bibcite{li2020fourier}{{43}{2020}{{Li et~al.}}{{Li, Kovachki, Azizzadenesheli, Liu, Bhattacharya, Stuart, and Anandkumar}}}
\bibcite{liu2023deepoheat}{{44}{2023}{{Liu et~al.}}{{Liu, Li, Hu, Yu, Shiau, Ai, Zeng, and Zhang}}}
\bibcite{long2024invertible}{{45}{2024}{{Long and Zhe}}{{}}}
\bibcite{mack2007surrogate}{{46}{2007}{{Mack et~al.}}{{Mack, Goel, Shyy, and Haftka}}}
\bibcite{mccormick1987multigrid}{{47}{1987}{{McCormick}}{{}}}
\bibcite{mcgreivy2024weak}{{48}{2024}{{McGreivy and Hakim}}{{}}}
\bibcite{NEURIPS2020_9332c513}{{49}{2020{a}}{{Moses and Churavy}}{{}}}
\bibcite{enzyme1}{{50}{2020{b}}{{Moses and Churavy}}{{}}}
\bibcite{enzyme2}{{51}{2021}{{Moses et~al.}}{{Moses, Churavy, Paehler, H\"{u}ckelheim, Narayanan, Schanen, and Doerfert}}}
\bibcite{enzyme3}{{52}{2022}{{Moses et~al.}}{{Moses, Narayanan, Paehler, Churavy, Schanen, H\"{u}ckelheim, Doerfert, and Hovland}}}
\bibcite{murray2018powerbox}{{53}{2018}{{Murray}}{{}}}
\bibcite{nusser1992tracing}{{54}{1992}{{Nusser and Dekel}}{{}}}
\bibcite{pathak2022fourcastnet}{{55}{2022}{{Pathak et~al.}}{{Pathak, Subramanian, Harrington, Raja, Chattopadhyay, Mardani, Kurth, Hall, Li, Azizzadenesheli, et~al.}}}
\bibcite{patro2015normalization}{{56}{2015}{{Patro}}{{}}}
\bibcite{potter2017pkdgrav3}{{57}{2017}{{Potter et~al.}}{{Potter, Stadel, and Teyssier}}}
\bibcite{prunet2008initial}{{58}{2008}{{Prunet et~al.}}{{Prunet, Pichon, Aubert, Pogosyan, Teyssier, and Gottloeber}}}
\bibcite{qian1999momentum}{{59}{1999}{{Qian}}{{}}}
\bibcite{rokhlin1985rapid}{{60}{1985}{{Rokhlin}}{{}}}
\bibcite{ronneberger2015u}{{61}{2015}{{Ronneberger et~al.}}{{Ronneberger, Fischer, and Brox}}}
\bibcite{schneider1995power}{{62}{1995}{{Schneider and Bartelmann}}{{}}}
\bibcite{schutz2003gravity}{{63}{2003}{{Schutz}}{{}}}
\bibcite{springel2021simulating}{{64}{2021}{{Springel et~al.}}{{Springel, Pakmor, Zier, and Reinecke}}}
\bibcite{thuerey2021physics}{{65}{2021}{{Thuerey et~al.}}{{Thuerey, Holl, Mueller, Schnell, Trost, and Um}}}
\bibcite{wang2014elucid}{{66}{2014}{{Wang et~al.}}{{Wang, Mo, Yang, Jing, and Lin}}}
\bibcite{wang2009minimal}{{67}{2009}{{Wang et~al.}}{{Wang, Moin, and Iaccarino}}}
\bibcite{zhang2019gradient}{{68}{2019}{{Zhang et~al.}}{{Zhang, He, Sra, and Jadbabaie}}}
\gdef \@abspage@last{73}
